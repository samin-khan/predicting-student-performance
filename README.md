# Multimodal Analysis of CourseKata Textbook Content

## Description
This project analyzes educational content, specifically from CourseKata textbooks and their associated chapter quizzes. It involves parsing HTML content, extracting text and image features from quiz pages, and then performing statistical analysis and model comparison to understand the impact of these multimodal features on metrics like R² score and Mean Absolute/Squared Error.

## Project Structure
```
├── data/ # Input data, intermediate CSVs, and final datasets
│   ├── chapter_quiz_page_content_all.csv
│   ├── chapter_quiz_with_multimodal_features.csv
│   └── filtered20_responses_2023.csv # Example initial input data
├── outputs/
│   └── images/ # Generated plots and visualizations
│       └── model_comparison_significance.png
├── parse_multimodal_features/ # Scripts for parsing and feature extraction
│   ├── data/ # (Potentially for local data used by parsing scripts)
│   ├── table_of_contents/ # HTML files for textbook table of contents
│   │   ├── ABC.html
│   │   └── ABCD.html
│   ├── parse_multimodal_features.ipynb # Extracts text/image features
│   └── parse_textbook_content.ipynb # Parses ToC and scrapes quiz content
├── .gitignore
├── analysis.ipynb # Main analysis, modeling, and result visualization
├── eda.ipynb # Exploratory Data Analysis
├── prompts.py # Python script, purpose may vary (e.g., LLM prompts)
└── requirements.txt # Python dependencies
```

## Data
The primary data sources are:
1.  **Textbook Structure**: HTML files of the table of contents for different CourseKata books (e.g., `ABC.html`, `ABCD.html`) located in `parse_multimodal_features/table_of_contents/`.
2.  **Chapter Quiz Content**: Scraped directly from CourseKata chapter quiz URLs. This includes text and image URLs.
3.  **Student Responses Data**: An initial dataset like `filtered20_responses_2023.csv` is used in `parse_textbook_content.ipynb` to identify unique pages for scraping.

Key processed data files generated and used:
-   `data/chapter_quiz_page_content_all.csv`: Contains aggregated text and image URLs for each chapter quiz from different books. Generated by `parse_textbook_content.ipynb`.
-   `data/chapter_quiz_with_multimodal_features.csv`: The above dataset augmented with extracted text and image features. Generated by `parse_multimodal_features.ipynb`.

## Feature Extraction
The project extracts the following features from chapter quiz content using `parse_multimodal_features.ipynb`:

### Text Features
-   **Average Word Length**: Mean length of words.
-   **Mean Word Rarity**: Based on Zipf frequency scores (inverted, so higher values indicate rarer words).
-   **Number of Complex Words**: Count of words with a Zipf frequency score below an empirical threshold (e.g., < 5.0).
-   **Total Word Count**: Total number of words.

### Image Features
-   **Average Lines per Image**: Average number of lines detected (using Hough Lines Transform via OpenCV) per image in a chapter's quizzes.
-   **Average Regions per Image**: Average number of contours detected (via OpenCV) per image.
-   **Total Images**: Total number of unique images in a chapter's quizzes.

## Analysis (`analysis.ipynb`)
The main analysis notebook (`analysis.ipynb`) performs the following:
-   Loads the processed data with multimodal features (likely `data/chapter_quiz_with_multimodal_features.csv`).
-   Builds and compares predictive models based on different feature sets:
    -   Base model
    -   Base + Image features
    -   Base + Text features
    -   Base + Image + Text features
-   Evaluates models using metrics such as R² Score and Mean Absolute Error (MAE) or Mean Squared Error (MSE).
-   Uses bootstrapping techniques to estimate confidence intervals and assess the statistical significance of performance differences between models.
-   Generates visualizations, such as bar plots comparing model performance, which are saved to `outputs/images/`.

## Setup

### Prerequisites
-   Python (the notebooks indicate use of Python 3.9)
-   Git

### Environment Setup
1.  **Clone the repository:**
    ```bash
    git clone <your-repository-url>
    cd "Datasci 194L Mini Project 2"
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python3 -m venv .venv
    source .venv/bin/activate  # On Windows use: .venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Selenium WebDriver (if re-running content scraping):**
    The `parse_multimodal_features/parse_textbook_content.ipynb` notebook uses Selenium with Chrome to scrape web content. If you intend to re-run this notebook:
    *   Ensure you have Google Chrome installed.
    *   You will need ChromeDriver. The script currently initializes `webdriver.Chrome()` which typically requires `chromedriver` to be in your system's PATH or managed via a library like `webdriver-manager` (if it were listed in `requirements.txt`). If you encounter issues, download the ChromeDriver version matching your Google Chrome browser and either add its location to your system's PATH or modify the script to point to the `chromedriver` executable.

## How to Run
The project consists of Jupyter notebooks that should generally be run in the following order:

1.  **Parse Textbook Content and Scrape Quizzes (Run this if `data/chapter_quiz_page_content_all.csv` needs to be generated or updated):**
    Open and run `parse_multimodal_features/parse_textbook_content.ipynb`. This will:
    *   Read initial data (e.g., `filtered20_responses_2023.csv`).
    *   Parse local HTML table of contents from `parse_multimodal_features/table_of_contents/`.
    *   Scrape chapter quiz pages using Selenium to extract text and image URLs.
    *   Save the output to `data/chapter_quiz_page_content_all.csv`.
    *   **Note:** This step involves web scraping and can be time-consuming. It requires the Selenium WebDriver setup mentioned above.

2.  **Extract Multimodal Features (Run this if `data/chapter_quiz_with_multimodal_features.csv` needs to be generated or updated):**
    Open and run `parse_multimodal_features/parse_multimodal_features.ipynb`. This will:
    *   Load `data/chapter_quiz_page_content_all.csv`.
    *   Extract text and image features for each chapter.
    *   Save the augmented dataset to `data/chapter_quiz_with_multimodal_features.csv`.

3.  **Exploratory Data Analysis (Recommended):**
    Open and run `eda.ipynb` to explore the datasets.

4.  **Main Analysis and Model Comparison:**
    Open and run `analysis.ipynb`. This notebook will:
    *   Load the data with features (likely `data/chapter_quiz_with_multimodal_features.csv`).
    *   Perform modeling, model comparison, and statistical significance testing.
    *   Generate and save result plots like `outputs/images/model_comparison_significance.png`.

## Key Dependencies
The `requirements.txt` file lists all Python dependencies. Key libraries include:
-   `pandas`
-   `numpy`
-   `matplotlib`
-   `seaborn`
-   `opencv-python` (cv2)
-   `requests`
-   `wordfreq`
-   `beautifulsoup4`
-   `selenium`
-   `jupyter` (or `notebook`, `jupyterlab`)
-   `scikit-learn` (implied for modeling and metrics)

## Results
The primary outputs of this project are:
-   A dataset (`data/chapter_quiz_with_multimodal_features.csv`) enriched with text and image features derived from CourseKata chapter quizzes.
-   A comparative analysis of predictive models, with visualizations (e.g., in `outputs/images/`) illustrating the performance (R² score, MAE/MSE) and statistical significance of incorporating different multimodal features.