{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install beautifulsoup4\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/d10c5tmd1b5g9_7g8dbmpt600000gp/T/ipykernel_92494/3163309484.py:1: DtypeWarning: Columns (35,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_20_test_features = pd.read_csv(\"../data/filtered20_responses_2023.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_20_test_features = pd.read_csv(\"../data/filtered20_responses_2023.csv\")\n",
    "df_unique_pages = df_20_test_features[['page', 'book']].drop_duplicates().reset_index(drop=True)\n",
    "abc_book_name = 'College / Statistics and Data Science (ABC)'\n",
    "abcd_book_name = 'College / Advanced Statistics and Data Science (ABCD)'\n",
    "df_unique_pages_abc = df_unique_pages[df_unique_pages.book == abc_book_name].reset_index(drop=True)\n",
    "df_unique_pages_abcd = df_unique_pages[df_unique_pages.book == abcd_book_name].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse HTML for table of contents for each book\n",
    "- Identify each div on webpage of book table of content that contains chapter review (i.e. quiz)\n",
    "\n",
    "### Currently Unused Features from Exercises. May Add Later\n",
    "- Loop through unique pages for that book and extract the URL to that page, and add it to df\n",
    "- Identify each div on webpage of book table of content that contains a page title or chapter review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chapter_quiz(path):\n",
    "    chapter_quiz_urls = {}\n",
    "    with open(path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div with class \"course-content\"\n",
    "    course_content_div = soup.find('div', class_='course-content')\n",
    "\n",
    "    # If the course_content_div is found, find all anchor tags within it\n",
    "    if course_content_div:\n",
    "        # Find all 'a' tags with an href starting with the specified pattern\n",
    "        import re\n",
    "        for link in course_content_div.find_all('a', href=True):\n",
    "            url = link['href']\n",
    "            page_name = link.get_text(strip=True)\n",
    "            if url.startswith(\"https://coursekata.org/preview/book/\"):\n",
    "                # Check if the page name contains \"Chapter X Review\" pattern\n",
    "                if re.search(r\"Chapter \\d+ Review\", page_name):\n",
    "                    chapter_num = re.search(r\"Chapter (\\d+)\", page_name).group(1)\n",
    "                    if chapter_num not in chapter_quiz_urls:\n",
    "                        chapter_quiz_urls[chapter_num] = [url]\n",
    "                    else:\n",
    "                        chapter_quiz_urls[chapter_num].append(url)\n",
    "    return chapter_quiz_urls\n",
    "\n",
    "# currently unused\n",
    "def parse_page_urls(path):\n",
    "    # import html file \n",
    "    with open(path, 'r') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div with class \"course-content\"\n",
    "    course_content_div = soup.find('div', class_='course-content')\n",
    "\n",
    "    # Initialize an empty dictionary to store the results\n",
    "    page_urls = {}\n",
    "\n",
    "    # If the course_content_div is found, find all anchor tags within it\n",
    "    if course_content_div:\n",
    "        # Find all 'a' tags with an href starting with the specified pattern\n",
    "        for link in course_content_div.find_all('a', href=True):\n",
    "            url = link['href']\n",
    "            if url.startswith(\"https://coursekata.org/preview/book/\"):\n",
    "                page_name = link.get_text(strip=True)\n",
    "                page_urls[page_name] = url\n",
    "    return page_urls\n",
    "\n",
    "# helper\n",
    "def add_urls_to_df(df, path):\n",
    "    page_urls = parse_page_urls(path)\n",
    "    df['url'] = None\n",
    "    for i, row in df.iterrows():\n",
    "        if row['page'] in page_urls:\n",
    "            df.loc[i, 'url'] = page_urls[row['page']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_abcd = \"table_of_contents/ABCD.html\"\n",
    "path_abc = \"table_of_contents/ABC.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_chapter_quiz_urls = get_chapter_quiz(path_abc)\n",
    "abcd_chapter_quiz_urls = get_chapter_quiz(path_abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currently unused\n",
    "# df_unique_pages_abcd = add_urls_to_df(df_unique_pages_abcd, path_abcd)\n",
    "# df_unique_pages_abc = add_urls_to_df(df_unique_pages_abc, path_abc)\n",
    "\n",
    "# df_unique_pages_abc.to_csv(\"data/df_pages_url_abc.csv\")\n",
    "# df_unique_pages_abcd.to_csv(\"data/df_pages_url_abcd.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Text + Images from Chapter Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_question_content(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Extract all visible text from the question area\n",
    "    # We'll get all text in .learnosity-item-page (excluding script/style/hidden/iframe/etc)\n",
    "    main_div = soup.find(\"div\", class_=\"learnosity-item-page\")\n",
    "    if not main_div:\n",
    "        # Fallback: use the whole document\n",
    "        main_div = soup\n",
    "\n",
    "    # Remove all script, style, iframe, and visually-hidden elements\n",
    "    for tag in main_div(['script', 'style', 'iframe', 'noscript', 'input']):\n",
    "        tag.decompose()\n",
    "    # Optionally remove 'sr-only' and aria-hidden text\n",
    "    for tag in main_div.find_all(class_=[\"sr-only\"]):\n",
    "        tag.decompose()\n",
    "    for tag in main_div.find_all(attrs={\"aria-hidden\": \"true\"}):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Get visible text\n",
    "    text = main_div.get_text(separator=' ', strip=True)\n",
    "\n",
    "    # Find all image URLs in the question\n",
    "    image_urls = [img['src'] for img in main_div.find_all('img') if img.get('src')]\n",
    "\n",
    "    return {\"text\": text, \"image_urls\": str(image_urls)}\n",
    "\n",
    "def agg_question_content_per_url(iframe_contents):\n",
    "    text = \"\"\n",
    "    image_urls = []\n",
    "\n",
    "    for iframe_id, html_body in iframe_contents.items():\n",
    "        result = extract_question_content(html_body)\n",
    "        text += f\"\\nQuestion ID: {iframe_id}\\n{result['text']}\"\n",
    "        image_urls.extend(eval(result['image_urls']))\n",
    "\n",
    "    # make image_urls unique\n",
    "    image_urls = list(set(image_urls))\n",
    "\n",
    "    return {\"text\": text, \"image_urls\": str(image_urls)}\n",
    "\n",
    "def combine_multiple_question_content(question_content_list):\n",
    "    if len(question_content_list) == 1:\n",
    "        return question_content_list[0]\n",
    "    text = \"\"\n",
    "    image_urls = []\n",
    "    for i in range(len(question_content_list)):\n",
    "        text += f\"\\nChapter Quiz Page {i+1}:\\n{question_content_list[i]['text']}\"\n",
    "        image_urls.extend(eval(question_content_list[i]['image_urls']))\n",
    "    image_urls = list(set(image_urls))\n",
    "    return {\"text\": text, \"image_urls\": str(image_urls)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def get_chapter_quiz_page_content(chapter_urls: dict):\n",
    "    page_content = {\n",
    "        \"chapter\" : [],\n",
    "        \"urls\" : [],\n",
    "        'text': [],\n",
    "        'images': [],\n",
    "    }\n",
    "\n",
    "    # 1. Launch browser\n",
    "    driver = webdriver.Chrome()  # or set options if needed\n",
    "    for chap, urls in chapter_urls.items():\n",
    "        if int(chap) > 9:\n",
    "            continue\n",
    "        \n",
    "        page_content['chapter'].append(chap)\n",
    "        page_content['urls'].append(urls)\n",
    "\n",
    "        # There may be multiple URLs for the same chapter\n",
    "        question_content_list = []\n",
    "        for i in range(len(urls)):\n",
    "            url = urls[i]\n",
    "            # 2. Load your page\n",
    "            driver.get(url)  # Replace with your actual URL\n",
    "\n",
    "            # 3. Wait for at least one iframe to appear (increase timeout if needed)\n",
    "            time.sleep(5)  # Explicitly wait for 5 seconds\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"iframe\"))\n",
    "            )\n",
    "\n",
    "            # 4. Gather all iframes\n",
    "            iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            iframe_contents = {}\n",
    "            for iframe in iframes:\n",
    "                iframe_id = iframe.get_attribute('id')\n",
    "                if iframe_id:\n",
    "                    # 5. Switch to this iframe\n",
    "                    driver.switch_to.frame(iframe)\n",
    "                    # 6. Wait for something meaningful to load, e.g. body > *\n",
    "                    WebDriverWait(driver, 20).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                    )\n",
    "                    # 7. Extract only the body content from the iframe\n",
    "                    body_element = driver.find_element(By.TAG_NAME, \"body\")\n",
    "                    body_html = body_element.get_attribute('innerHTML')\n",
    "                    \n",
    "                    # Store the full iframe HTML in the dictionary for reference if needed\n",
    "                    iframe_contents[iframe_id] = body_html\n",
    "                    \n",
    "                    # 8. Switch back to the main page before handling the next iframe\n",
    "                    driver.switch_to.default_content()\n",
    "            print(f\"Combined HTML from {len(iframes)} iframes in chapter {chap}\")\n",
    "\n",
    "            # Parse out text and image content per URL\n",
    "            question_content_per_url = agg_question_content_per_url(iframe_contents)\n",
    "            question_content_list.append(question_content_per_url)\n",
    "\n",
    "        # Consolidate text and image content per chapter\n",
    "        question_content = combine_multiple_question_content(question_content_list)\n",
    "        page_content['text'].append(question_content['text'])\n",
    "        page_content['images'].append(question_content['image_urls'])\n",
    "        page_content['images'] = list(set(page_content['images']))\n",
    "    driver.quit()\n",
    "\n",
    "    return page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined HTML from 16 iframes in chapter 1\n",
      "Combined HTML from 22 iframes in chapter 2\n",
      "Combined HTML from 18 iframes in chapter 2\n",
      "Combined HTML from 24 iframes in chapter 3\n",
      "Combined HTML from 16 iframes in chapter 3\n",
      "Combined HTML from 23 iframes in chapter 4\n",
      "Combined HTML from 17 iframes in chapter 4\n",
      "Combined HTML from 23 iframes in chapter 5\n",
      "Combined HTML from 17 iframes in chapter 5\n",
      "Combined HTML from 23 iframes in chapter 6\n",
      "Combined HTML from 14 iframes in chapter 6\n",
      "Combined HTML from 15 iframes in chapter 7\n",
      "Combined HTML from 23 iframes in chapter 8\n",
      "Combined HTML from 23 iframes in chapter 9\n",
      "Combined HTML from 14 iframes in chapter 9\n",
      "Combined HTML from 21 iframes in chapter 10\n",
      "Combined HTML from 22 iframes in chapter 10\n",
      "Combined HTML from 18 iframes in chapter 11\n",
      "Combined HTML from 23 iframes in chapter 11\n",
      "Combined HTML from 24 iframes in chapter 12\n",
      "Combined HTML from 23 iframes in chapter 12\n",
      "Combined HTML from 16 iframes in chapter 1\n",
      "Combined HTML from 22 iframes in chapter 2\n",
      "Combined HTML from 18 iframes in chapter 2\n",
      "Combined HTML from 24 iframes in chapter 3\n",
      "Combined HTML from 16 iframes in chapter 3\n",
      "Combined HTML from 23 iframes in chapter 4\n",
      "Combined HTML from 17 iframes in chapter 4\n",
      "Combined HTML from 23 iframes in chapter 5\n",
      "Combined HTML from 17 iframes in chapter 5\n",
      "Combined HTML from 23 iframes in chapter 6\n",
      "Combined HTML from 14 iframes in chapter 6\n",
      "Combined HTML from 15 iframes in chapter 7\n",
      "Combined HTML from 23 iframes in chapter 8\n",
      "Combined HTML from 23 iframes in chapter 9\n",
      "Combined HTML from 14 iframes in chapter 9\n",
      "Combined HTML from 21 iframes in chapter 10\n",
      "Combined HTML from 22 iframes in chapter 10\n",
      "Combined HTML from 18 iframes in chapter 11\n",
      "Combined HTML from 23 iframes in chapter 11\n",
      "Combined HTML from 24 iframes in chapter 12\n",
      "Combined HTML from 23 iframes in chapter 12\n"
     ]
    }
   ],
   "source": [
    "abc_chapter_quiz_page_content = get_chapter_quiz_page_content(abc_chapter_quiz_urls)\n",
    "abcd_chapter_quiz_page_content = get_chapter_quiz_page_content(abcd_chapter_quiz_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_book_name = 'College / Statistics and Data Science (ABC)'\n",
    "abcd_book_name = 'College / Advanced Statistics and Data Science (ABCD)'\n",
    "\n",
    "df_abc_chapter_quiz_page_content = pd.DataFrame(abc_chapter_quiz_page_content)\n",
    "df_abcd_chapter_quiz_page_content = pd.DataFrame(abcd_chapter_quiz_page_content)\n",
    "\n",
    "df_abc_chapter_quiz_page_content['book'] = abc_book_name\n",
    "df_abcd_chapter_quiz_page_content['book'] = abcd_book_name\n",
    "\n",
    "df_chapter_quiz_page_content_all = pd.concat([df_abc_chapter_quiz_page_content, df_abcd_chapter_quiz_page_content]).reset_index(drop=True)\n",
    "df_chapter_quiz_page_content_all.to_csv(\"data/chapter_quiz_page_content_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Text + Images from Exercises (currently unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_page_text_and_images(url):\n",
    "    # request page\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    student_content = soup.select_one('div.course-content.student')\n",
    "\n",
    "    # Find all img and alt text tags\n",
    "    img_tags = soup.find_all('img')\n",
    "    alt_text_and_images = []\n",
    "    images_only = []\n",
    "    for img in img_tags:\n",
    "        alt_text_and_images.append((img.get('alt', ''), img.get('src', '')))\n",
    "        images_only.append(img.get('src', ''))\n",
    "\n",
    "    page_content = {\n",
    "        'text': student_content.text.strip() if student_content else None,\n",
    "        'images_only': str(images_only),\n",
    "        'alt_text_and_images': str(alt_text_and_images)\n",
    "    }\n",
    "\n",
    "    return page_content\n",
    "\n",
    "def get_page_content_list_chapter_urls(chapter_urls: dict):\n",
    "    page_content = {\n",
    "        \"chapter\" : [],\n",
    "        \"urls\" : [],\n",
    "        'text': [],\n",
    "        'images_only': [],\n",
    "        'alt_text_and_images': []\n",
    "    }\n",
    "    for chap, urls in chapter_urls.items():\n",
    "        page_content['chapter'].append(chap)\n",
    "        page_content['urls'].append(urls)\n",
    "        for i in range(len(urls)):\n",
    "            url = urls[i]\n",
    "            url_page_content = get_page_text_and_images(url)\n",
    "            page_num = i + 1 \n",
    "            page_content['text'] = f\"\\nChapter Quiz Page {page_num}:\\n\" + url_page_content['text']\n",
    "            if page_num == 1:\n",
    "                page_content['images_only'] = url_page_content['images_only']\n",
    "                page_content['alt_text_and_images'] = url_page_content['alt_text_and_images']\n",
    "            else:\n",
    "                page_content['images_only'] = str(eval(page_content['images_only']) + eval(url_page_content['images_only']))\n",
    "                page_content['alt_text_and_images'] = str(eval(page_content['alt_text_and_images']) + eval(url_page_content['alt_text_and_images']))\n",
    "\n",
    "    return page_content\n",
    "\n",
    "\n",
    "def add_page_content_to_df(df):\n",
    "    df['page_context_text'] = None\n",
    "    df['page_context_alt_text_and_images'] = None\n",
    "    for i, row in df.iterrows():\n",
    "        if  row['url']:\n",
    "            page_content = get_page_text_and_images(row['url'])\n",
    "            df.loc[i, 'page_context_text'] = page_content['text']\n",
    "            df.loc[i, 'page_context_images_only'] = page_content['images_only']\n",
    "            df.loc[i, 'page_context_alt_text_and_images'] = page_content['alt_text_and_images']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_chapter_quiz_page_content = get_page_content_list_chapter_urls(abc_chapter_quiz_urls)\n",
    "abcd_chapter_quiz_page_content = get_page_content_list_chapter_urls(abc_chapter_quiz_urls)\n",
    "\n",
    "df_abc_chapter_quiz_page_content = pd.DataFrame(abc_chapter_quiz_page_content)\n",
    "df_abcd_chapter_quiz_page_content = pd.DataFrame(abcd_chapter_quiz_page_content)\n",
    "\n",
    "df_abc_chapter_quiz_page_content['book'] = abc_book_name\n",
    "df_abcd_chapter_quiz_page_content['book'] = abcd_book_name\n",
    "\n",
    "df_chapter_quiz_page_content_all = pd.concat([df_abc_chapter_quiz_page_content, df_abcd_chapter_quiz_page_content]).reset_index(drop=True)\n",
    "df_chapter_quiz_page_content_all.to_csv(\"data/chapter_quiz_page_content_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_pages_abc = add_page_content_to_df(df_unique_pages_abc)\n",
    "df_unique_pages_abcd = add_page_content_to_df(df_unique_pages_abcd)\n",
    "\n",
    "df_unique_pages_abc.to_csv(\"data/df_pages_content_abc.csv\")\n",
    "df_unique_pages_abcd.to_csv(\"data/df_pages_content_abcd.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "educ139-project-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
