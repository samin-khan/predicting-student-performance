{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I was originally doing the predicting itself with LLM but it was taking too long / too expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ: “Does incorporating textbook content features, such as text and images, improve quiz score prediction beyond past student performance on exercises?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features = pd.read_csv(\"data/filtered80_responses_2023.csv\")\n",
    "df_80_validate_eoc = pd.read_csv(\"data/filtered80_eoc_2023.csv\")\n",
    "\n",
    "df_unique_pages_abc = pd.read_csv(\"parse_textbook_content/data/df_pages_content_abc.csv\")\n",
    "df_unique_pages_abcd = pd.read_csv(\"parse_textbook_content/data/df_pages_content_abcd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features.chapter_num.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_pages = pd.concat([df_unique_pages_abc, df_unique_pages_abcd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to get all features\n",
    "book_abc = 'College / Statistics and Data Science (ABC)'\n",
    "book_abcd = 'College / Advanced Statistics and Data Science (ABCD)'\n",
    "\n",
    "df_80_validate_features = df_80_validate_features.merge(\n",
    "    df_unique_pages,\n",
    "    on=['book', 'page'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# past performance only features\n",
    "past_performance_cols = ['class_id', 'student_id', 'chapter', 'page', 'item_id', 'item_type', 'points_earned', 'points_possible', 'attempt']\n",
    "\n",
    "# past performance + text\n",
    "past_performance_plus_text_cols = ['class_id', 'student_id', 'chapter', 'page', 'item_id', 'item_type', 'points_earned', 'points_possible', 'attempt', 'lrn_response_json', 'attempt', 'page_context_text']\n",
    "\n",
    "# past performance + images\n",
    "past_performance_plus_image_cols = ['class_id', 'student_id', 'chapter', 'page', 'item_id', 'item_type', 'points_earned', 'points_possible', 'attempt', 'page_context_images_only']\n",
    "# why \n",
    "# past performance + images + text\n",
    "past_performance_plus_text_and_image_cols = ['class_id', 'student_id', 'chapter', 'page', 'item_id', 'item_type', 'points_earned', 'points_possible', 'attempt', 'lrn_response_json', 'attempt', 'page_context_text', 'page_context_alt_text_and_images']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompts at Chapter Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class score_prediction(BaseModel):\n",
    "    explanation: str = Field(..., description=\"Explanation of your prediction.\")\n",
    "    prediction: float = Field(\n",
    "        ..., \n",
    "        description=\"Final estimated probability (0-1) a random US adult answers this specific question correctly, integrating all visual, textual, and interaction factors up to 4 decimal places.\"\n",
    "    )\n",
    "\n",
    "USER_PROMPT_END = f\"\"\"\\nBased on this information, explain your reasoning and predict the probability (between 0 and 1) with 4 decimal places that this student will answer a randomly selected question from the end-of-chapter quiz correctly.\n",
    "\n",
    "Respond in the following JSON format, conforming to the `score_prediction` schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"explanation\": ...\n",
    "  \"prediction\": ...\n",
    "}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define system prompts from our prompts.py file\n",
    "from prompts import (\n",
    "    system_prompt_past_performance_only,\n",
    "    system_prompt_past_performance_plus_text,\n",
    "    system_prompt_past_performance_plus_image,\n",
    "    system_prompt_past_performance_plus_text_and_image\n",
    ")\n",
    "\n",
    "# Helper function to clean text\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "# Helper function to create a single exercise entry\n",
    "def create_chapter_agg_prompt(df_chapter, include_text=False, include_image=False):\n",
    "    user_prompt = \"\"\n",
    "    for _, row in df_chapter.iterrows():\n",
    "        base = f\"\\nChapter: {row['chapter']}\\nPage Name: {row['page']}\\nItem ID: {row['item_id']}\\nItem Type: {row['item_type']}\\nStudent earned {row['points_earned']} out of {row['points_possible']}.\"\n",
    "        \n",
    "        if include_text and include_image:\n",
    "            if not pd.isna(row['page_context_text']):\n",
    "                base += \"\\nPage Context:\\n\"\n",
    "                text = clean_text(row.get('page_context_text', ''))\n",
    "                if text:\n",
    "                    base += text\n",
    "\n",
    "            if not pd.isna(row['page_context_alt_text_and_images']):\n",
    "                page_context_alt_text_and_images = eval(row.get('page_context_alt_text_and_images', ''))\n",
    "                for image in page_context_alt_text_and_images:\n",
    "                    if image[1].startswith('https://'):\n",
    "                        base += f\"\\nImage Alt Text: {image[0]}\"\n",
    "\n",
    "        elif include_text :\n",
    "            if not pd.isna(row['page_context_text']):\n",
    "                base += \"\\nPage Context:\\n\"\n",
    "                text = clean_text(row.get('page_context_text', ''))\n",
    "                if text:\n",
    "                    base += text\n",
    "        \n",
    "        elif include_image:\n",
    "            if not pd.isna(row['page_context_images_only']):\n",
    "                base += \"\\nSee images attached\\n\"\n",
    "\n",
    "        user_prompt += base\n",
    "    \n",
    "    return user_prompt\n",
    "\n",
    "def create_agg_image_url_list(df_chapter):\n",
    "    image_url_list = []\n",
    "    for _, row in df_chapter.iterrows():\n",
    "        if not pd.isna(row['page_context_images_only']):\n",
    "            curr_urls = eval(row['page_context_images_only'])\n",
    "            for url in curr_urls:\n",
    "                if url.startswith('https://') and (url.endswith('.png') or url.endswith('.jpg') or url.endswith('.jpeg')):\n",
    "                    image_url_list.append(url)\n",
    "    return list(set(image_url_list))\n",
    "\n",
    "def get_agg_list_scores(df_chapter):\n",
    "    scores = []\n",
    "    for _, row in df_chapter.iterrows():\n",
    "        score = row['points_earned'] / row['points_possible']\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def generate_chapter_prompts_and_image_lists(df):\n",
    "    grouped = df.groupby(['class_id', 'student_id', 'book', 'chapter_num'])\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for (class_id, student_id, book, chapter_num), group in grouped:\n",
    "        # Sort by page for consistency\n",
    "        group_sorted = group.sort_values(by=['page_num', 'item_id'])\n",
    "\n",
    "        # Build each of the 4 prompt types using the new helper\n",
    "        prompt_perf_only = create_chapter_agg_prompt(\n",
    "            group_sorted, include_text=False, include_image=False\n",
    "        )\n",
    "\n",
    "        prompt_plus_text = create_chapter_agg_prompt(\n",
    "            group_sorted, include_text=True, include_image=False\n",
    "        )\n",
    "\n",
    "        prompt_plus_image = create_chapter_agg_prompt(\n",
    "            group_sorted, include_text=False, include_image=True\n",
    "        )\n",
    "\n",
    "        prompt_plus_text_and_image = create_chapter_agg_prompt(\n",
    "            group_sorted, include_text=True, include_image=True\n",
    "        )\n",
    "\n",
    "        image_url_list = str(create_agg_image_url_list(group_sorted))\n",
    "\n",
    "        scores = np.mean(group_sorted['points_earned'] / group_sorted['points_possible']) # get_agg_list_scores(group_sorted)\n",
    "\n",
    "        results.append({\n",
    "            'class_id': class_id,\n",
    "            'student_id': student_id,\n",
    "            'book': book,\n",
    "            'chapter': chapter_num,\n",
    "            'Prompt_PastPerformanceOnly': prompt_perf_only,\n",
    "            'Prompt_PastPerformancePlusText': prompt_plus_text,\n",
    "            'Prompt_PastPerformancePlusImage': prompt_plus_image,\n",
    "            'Prompt_PastPerformancePlusTextAndImage': prompt_plus_text_and_image,\n",
    "            'image_url_list': image_url_list,\n",
    "            'avg_exercise_scores': scores\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def create_full_prompt_row(row, include_text=False, include_image=False):\n",
    "    if include_text and include_image:\n",
    "        system_prompt = system_prompt_past_performance_plus_text_and_image\n",
    "        user_prompt = row['Prompt_PastPerformancePlusTextAndImage']\n",
    "    elif include_text:\n",
    "        system_prompt = system_prompt_past_performance_plus_text\n",
    "        user_prompt = row['Prompt_PastPerformancePlusText']\n",
    "    elif include_image:\n",
    "        system_prompt = system_prompt_past_performance_plus_image\n",
    "        user_prompt = row['Prompt_PastPerformancePlusImage']\n",
    "    else:\n",
    "        system_prompt = system_prompt_past_performance_only\n",
    "        user_prompt = row['Prompt_PastPerformanceOnly']\n",
    "\n",
    "    # Add the end of the prompt\n",
    "    user_prompt += USER_PROMPT_END\n",
    "\n",
    "    # full prompt with code\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}]}\n",
    "    ]\n",
    "    if include_image and type(row['image_url_list']) == str:\n",
    "        for image_url in eval(row['image_url_list']):\n",
    "            if image_url.startswith('https://'):\n",
    "                messages[1]['content'].append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": image_url,\n",
    "                    },\n",
    "                })\n",
    "\n",
    "    return str(messages)\n",
    "\n",
    "def create_full_prompt_df(df):\n",
    "    df['FullPrompt_PastPerformanceOnly'] = df.apply(lambda x: create_full_prompt_row(x, include_text=False, include_image=False), axis=1)\n",
    "    df['FullPrompt_PastPerformancePlusText'] = df.apply(lambda x: create_full_prompt_row(x, include_text=True, include_image=False), axis=1)\n",
    "    df['FullPrompt_PastPerformancePlusImage'] = df.apply(lambda x: create_full_prompt_row(x, include_text=False, include_image=True), axis=1)\n",
    "    df['FullPrompt_PastPerformancePlusTextAndImage'] = df.apply(lambda x: create_full_prompt_row(x, include_text=True, include_image=True), axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to call GPT-4.1 Nano with image capabilities\n",
    "def generate_eoc_predictions(row, type, model = \"gpt-4.1-nano\"):\n",
    "    messages = eval(row[f'FullPrompt_{type}'])\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=score_prediction,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out chapter 2, 3 6\n",
    "df_80_validate_features_filter = df_80_validate_features[~df_80_validate_features.chapter_num.isin([2, 3, 6])]\n",
    "df_80_validate_eoc_filter = df_80_validate_eoc[~df_80_validate_eoc.chapter.isin([2, 3, 6])]\n",
    "\n",
    "\n",
    "df_80_validate_features_filter_sample = df_80_validate_features_filter.groupby(['chapter_num']).apply(lambda x: x.sample(2)).reset_index(drop=True)\n",
    "df_80_validate_eoc_filter_sample = df_80_validate_eoc_filter.groupby(['chapter']).apply(lambda x: x.sample(2)).reset_index(drop=True)\n",
    "\n",
    "# sample from each chapter\n",
    "sample = 2\n",
    "df_80_validate_features_filter_sample = df_80_validate_features_filter.groupby(['chapter']).apply(lambda x: x.sample(sample)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_filter_sample = generate_chapter_prompts_and_image_lists(df_80_validate_features_filter_sample)\n",
    "df_80_validate_features_filter_sample = create_full_prompt_df(df_80_validate_features_filter_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for each prompt\n",
    "df_80_validate_features_filter_sample['prediction_PastPerformanceOnly'] = \"\"\n",
    "df_80_validate_features_filter_sample['prediction_PastPerformancePlusText'] = \"\"\n",
    "df_80_validate_features_filter_sample['prediction_PastPerformancePlusImage'] = \"\"\n",
    "df_80_validate_features_filter_sample['prediction_PastPerformancePlusTextAndImage'] = \"\"\n",
    "\n",
    "for i in range(len(df_80_validate_features_filter_sample)):\n",
    "    df_80_validate_features_filter_sample.loc[i, 'prediction_PastPerformanceOnly'] = generate_eoc_predictions(df_80_validate_features_filter_sample.iloc[i], \"PastPerformanceOnly\")\n",
    "    df_80_validate_features_filter_sample.loc[i, 'prediction_PastPerformancePlusText'] = generate_eoc_predictions(df_80_validate_features_filter_sample.iloc[i], \"PastPerformancePlusText\")\n",
    "    df_80_validate_features_filter_sample.loc[i, 'prediction_PastPerformancePlusImage'] = generate_eoc_predictions(df_80_validate_features_filter_sample.iloc[i], \"PastPerformancePlusImage\")\n",
    "    df_80_validate_features_filter_sample.loc[i, 'prediction_PastPerformancePlusTextAndImage'] = generate_eoc_predictions(df_80_validate_features_filter_sample.iloc[i], \"PastPerformancePlusTextAndImage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_filter_sample.merge(df_80_validate_eoc_filter, on=['class_id', 'student_id', 'book', 'chapter'], how='left')\n",
    "# Convert prediction columns from string to float\n",
    "for col in ['prediction_PastPerformanceOnly', 'prediction_PastPerformancePlusText', \n",
    "            'prediction_PastPerformancePlusImage', 'prediction_PastPerformancePlusTextAndImage']:\n",
    "    # Extract the prediction value from the JSON string\n",
    "    df_80_validate_features_filter_sample[col + \"_score\"] = df_80_validate_features_filter_sample[col].apply(\n",
    "        lambda x: float(eval(x)[\"prediction\"]) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# Fix the merge issue by converting chapter types\n",
    "df_80_validate_features_filter_sample['chapter'] = df_80_validate_features_filter_sample['chapter'].astype(int)\n",
    "df_80_validate_eoc_filter['chapter'] = df_80_validate_eoc_filter['chapter'].astype(int)\n",
    "\n",
    "# Merge the dataframes correctly\n",
    "merged_df = df_80_validate_features_filter_sample.merge(\n",
    "    df_80_validate_eoc_filter, \n",
    "    on=['class_id', 'student_id', 'book', 'chapter'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate differences for each prediction type\n",
    "merged_df['diff_PastPerformanceOnly'] = merged_df['prediction_PastPerformanceOnly_score'] - merged_df['score']\n",
    "merged_df['diff_PastPerformancePlusText'] = merged_df['prediction_PastPerformancePlusText_score'] - merged_df['score']\n",
    "merged_df['diff_PastPerformancePlusImage'] = merged_df['prediction_PastPerformancePlusImage_score'] - merged_df['score']\n",
    "merged_df['diff_PastPerformancePlusTextAndImage'] = merged_df['prediction_PastPerformancePlusTextAndImage_score'] - merged_df['score']\n",
    "\n",
    "# Calculate average mean difference for each category\n",
    "avg_diff_past_only = merged_df['diff_PastPerformanceOnly'].abs().mean()\n",
    "avg_diff_past_text = merged_df['diff_PastPerformancePlusText'].abs().mean()\n",
    "avg_diff_past_image = merged_df['diff_PastPerformancePlusImage'].abs().mean()\n",
    "avg_diff_past_text_image = merged_df['diff_PastPerformancePlusTextAndImage'].abs().mean()\n",
    "\n",
    "print(\"Average Mean Absolute Differences:\")\n",
    "print(f\"Past Performance Only: {avg_diff_past_only:.4f}\")\n",
    "print(f\"Past Performance + Text: {avg_diff_past_text:.4f}\")\n",
    "print(f\"Past Performance + Image: {avg_diff_past_image:.4f}\")\n",
    "print(f\"Past Performance + Text + Image: {avg_diff_past_text_image:.4f}\")\n",
    "\n",
    "# Update the dataframe for further analysis\n",
    "df_80_validate_features_filter_sample = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(y)[\"prediction\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_added_features = generate_chapter_prompts_and_image_lists(df_80_validate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_added_features_and_eoc = df_80_validate_features_added_features.merge(df_80_validate_eoc, on=['class_id', 'student_id', 'book', 'chapter'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_added_features_and_eoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_80_validate_features_added_features_and_eoc[df_80_validate_features_added_features_and_eoc.index ==4855].to_csv(\"outputs/df_80_validate_features_added_features_and_eoc_4855.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 columns for number of images and number of words in text\n",
    "df_80_validate_features_added_features_and_eoc['num_images'] = df_80_validate_features_added_features_and_eoc['page_context_images_only'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "df_80_validate_features_added_features_and_eoc['num_words'] = df_80_validate_features_added_features_and_eoc['page_context_text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "\n",
    "\n",
    "df_80_validate_features_added_features_and_eoc.num_words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a regression model to predict the score from the features\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# features: avg_exercise_scores\n",
    "# predict: score\n",
    "\n",
    "# Filter out rows with missing scores\n",
    "df_train = df_80_validate_features_added_features_and_eoc.dropna(subset=['score', 'avg_exercise_scores'])\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_train[['avg_exercise_scores']]\n",
    "y = df_train['score']\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print model coefficients and score\n",
    "print(\"Model coefficients:\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(f\"Avg Exercise Scores: {model.coef_[0]:.4f}\")\n",
    "# print(f\"Number of Images: {model.coef_[1]:.4f}\")\n",
    "# print(f\"Number of Words: {model.coef_[2]:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "train_score = model.score(X, y)\n",
    "print(f\"R² score on training data: {train_score:.4f}\")\n",
    "\n",
    "# Make predictions on training data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average score over all the chapters\n",
    "df_80_validate_features_added_features_and_eoc.score.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_20_test_features to only include rows where the combination of student_id, class_id, and chapter exists in df_20_test_eoc\n",
    "df_20_test_features_removed_rows = df_20_test_features.merge(\n",
    "    df_20_test_eoc[['student_id', 'class_id', 'chapter']],\n",
    "    on=['student_id', 'class_id', 'chapter'],\n",
    "    how='right'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use that model to predict the score for the test set\n",
    "x_test = df_20_test_features_removed_rows[['avg_exercise_scores']].fillna(model.coef_[0]) # fillna with the average score over all the chapters\n",
    "\n",
    "y_pred_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to a DataFrame with id and score columns\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': range(len(y_pred_test)),\n",
    "    'score': y_pred_test\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "predictions_df.to_csv(\"outputs/df_20_test_features_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_test_features_raw = pd.read_csv(\"data/filtered20_responses_2023.csv\")\n",
    "df_20_test_eoc = pd.read_csv(\"data/filtered20_eoc_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge EOC data with page content for ABC textbook\n",
    "df_20_test_features_raw = df_20_test_features_raw.merge(\n",
    "    df_unique_pages,\n",
    "    on=['book', 'page'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_20_test_features = generate_chapter_prompts_and_image_lists(df_20_test_features_raw)\n",
    "# df_20_test_features = create_full_prompt_df(df_20_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for each prompt\n",
    "df_20_test_features['prediction_PastPerformanceOnly'] = \"\"\n",
    "df_20_test_features['prediction_PastPerformancePlusText'] = \"\"\n",
    "df_20_test_features['prediction_PastPerformancePlusImage'] = \"\"\n",
    "df_20_test_features['prediction_PastPerformancePlusTextAndImage'] = \"\"\n",
    "\n",
    "print(\"Each df has rows: \", len(df_20_test_features))\n",
    "# print(\"Total API calls needed: \", len(df_20_test_features) * 4)\n",
    "for i in range(len(df_20_test_features)):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"On API call: {i} out of {len(df_20_test_features) * 4}\")\n",
    "    df_20_test_features.loc[i, 'prediction_PastPerformanceOnly'] = generate_eoc_predictions(df_20_test_features.iloc[i], \"PastPerformanceOnly\")\n",
    "    df_20_test_features.loc[i, 'prediction_PastPerformancePlusText'] = generate_eoc_predictions(df_20_test_features.iloc[i], \"PastPerformancePlusText\")\n",
    "    df_20_test_features.loc[i, 'prediction_PastPerformancePlusImage'] = generate_eoc_predictions(df_20_test_features.iloc[i], \"PastPerformancePlusImage\")\n",
    "    df_20_test_features.loc[i, 'prediction_PastPerformancePlusTextAndImage'] = generate_eoc_predictions(df_20_test_features.iloc[i], \"PastPerformancePlusTextAndImage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20_test_features.to_csv(\"outputs/df_20_test_features_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 columns for number of images and number of words in text\n",
    "df_20_test_features['num_images'] = df_20_test_features['page_context_images_only'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "df_20_test_features['num_words'] = df_20_test_features['page_context_text'].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.read_csv(\"outputs/df_20_test_features_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
